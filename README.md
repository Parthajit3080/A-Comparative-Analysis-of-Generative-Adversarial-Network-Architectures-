# A-Comparative-Analysis-of-Generative-Adversarial-Network-Architectures-


## 🧠 Overview
<p align="justify">
This project presents a detailed comparative study of **Vanilla GAN**, **Wasserstein GAN (WGAN)**, and **WGAN with Gradient Penalty (WGAN-GP)**, implemented across datasets of increasing complexity—**MNIST**, **CIFAR-10**, and **Anime Faces**.  
The goal is to analyze **training stability**, **sample quality**, and **theoretical robustness** of each architecture. 
</p>

---

## 🚀 Technologies Used
- Python 3.9  
- PyTorch  
- Torchvision  
- Matplotlib  
- NumPy  
- Scikit-learn  
- Frechet Inception Distance (torchmetrics)  
- Google Colab  
- Kaggle  

---

## 📊 Evaluation Metrics
- **Fréchet Inception Distance (FID)**  
- **Inception Score (IS)**  
- **Visual Fidelity**  
- **Loss Dynamics**

---

## 🖼 Sample Outputs
Add a few generated images from each model and dataset:  
- MNIST  
- CIFAR-10  
- Anime Faces

---

## 📈 Key Insights
- ✅ WGAN-GP shows the best performance in terms of **training stability** and **image quality**.  
- ⚠️ Vanilla GAN is prone to **mode collapse** and **unstable gradients**.  
- 📉 FID score **correlates well with visual quality** across all datasets.

---

## 📄 Full Report
https://docs.google.com/document/d/14ILdxdc2qljDoKtwGzHv4sGrYAeMZnjfXZrNbiJSzLc/edit?usp=sharing

---

## 👨‍💻 Authors
- **Parthajit Das**  
- **Souvick Roy**  
- **Aditya Agarwal**

