# A-Comparative-Analysis-of-Generative-Adversarial-Network-Architectures-


## ğŸ§  Overview
<p align="justify">
This project presents a detailed comparative study of **Vanilla GAN**, **Wasserstein GAN (WGAN)**, and **WGAN with Gradient Penalty (WGAN-GP)**, implemented across datasets of increasing complexityâ€”**MNIST**, **CIFAR-10**, and **Anime Faces**.  
The goal is to analyze **training stability**, **sample quality**, and **theoretical robustness** of each architecture. 
</p>

---

## ğŸš€ Technologies Used
- Python 3.9  
- PyTorch  
- Torchvision  
- Matplotlib  
- NumPy  
- Scikit-learn  
- Frechet Inception Distance (torchmetrics)  
- Google Colab  
- Kaggle  

---

## ğŸ“Š Evaluation Metrics
- **FrÃ©chet Inception Distance (FID)**  
- **Inception Score (IS)**  
- **Visual Fidelity**  
- **Loss Dynamics**

---

## ğŸ–¼ Sample Outputs
Add a few generated images from each model and dataset:  
- MNIST  
- CIFAR-10  
- Anime Faces

---

## ğŸ“ˆ Key Insights
- âœ… WGAN-GP shows the best performance in terms of **training stability** and **image quality**.  
- âš ï¸ Vanilla GAN is prone to **mode collapse** and **unstable gradients**.  
- ğŸ“‰ FID score **correlates well with visual quality** across all datasets.

---

## ğŸ“„ Full Report
https://docs.google.com/document/d/14ILdxdc2qljDoKtwGzHv4sGrYAeMZnjfXZrNbiJSzLc/edit?usp=sharing

---

## ğŸ‘¨â€ğŸ’» Authors
- **Parthajit Das**  
- **Souvick Roy**  
- **Aditya Agarwal**

